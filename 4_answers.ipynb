{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adv_1_answers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu1g0fOTO5wE",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5V4Skp-O-6y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqkq51EhteSW",
        "colab_type": "text"
      },
      "source": [
        "# Customizable Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDgE2W6wO8pu",
        "colab_type": "text"
      },
      "source": [
        "Let's dive into creating a neural network using all customization that it offers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjRSqTzXO3rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from tensorflow.nn import relu, softmax\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoNqTDL-bufj",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 : Download and prepare the data\n",
        "For this task, let's use the CIFAR-10 dataset created by Krizhevsky *et al*. It consists of the following 10 classes : \"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\",\t\"Horse\", \"Ship\",\t\"Truck\". Each image in the dataset is a 3x32x32 color image. The 3 here stands for the three color channels (RBG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TXYLZFcbk2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_dataset = tfds.load('cifar10', as_supervised=True)\n",
        "\n",
        "def cifar_preprocess(image, label):\n",
        "  image = image/255\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiZJDrQD8MeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_train = cifar_dataset['train'].map(cifar_preprocess).shuffle(10000).batch(128)\n",
        "cifar_test = cifar_dataset['test'].map(cifar_preprocess).batch(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX1oquCzdNiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Class Names\n",
        "class_names = {\n",
        " 0: 'Airplane',\n",
        " 1: 'Automoble',\n",
        " 2: 'Bird',\n",
        " 3: 'Cat',\n",
        " 4: 'Deer',\n",
        " 5: 'Dog',\n",
        " 6: 'Frog',\n",
        " 7: 'Horse',\n",
        " 8: 'Ship',\n",
        " 9: 'Truck'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1YAV1_YdAQK",
        "colab_type": "text"
      },
      "source": [
        "Let's preview a couple images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vp64_ff4iGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(image, label):\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  plt.title(label)\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY6e_pEOcus_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image, label in cifar_train.take(2):\n",
        "  display(image[0], class_names[label[0].numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va4V1cShduyE",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 : Create the custom neural network\n",
        "For this tutorial, you will be working with the Sub-Classing API of TensorFlow. In-order to show the flexibility and cutomisation capabilties of TensorFlow2.0, let's break this up into multiple parts. The first step is to create your own Dense Layer, with an activation, a bias, and a kernal initilization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kedI4JG414cv",
        "colab_type": "text"
      },
      "source": [
        "### Activation function\n",
        "You can create our own activation using two methods in TensorFlow, the first is by extending the Layer class, and the second is by creating a simple python function to perform the task. Since you will see an example of extending a layer further in the tutorial, let's write a simple pythonic function. Let's create a LeakyReLU activation for our network.\n",
        "\n",
        "TODO:\n",
        "  Write your own LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsE9lyIGThP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_leaky_relu(x, alpha = 0.01): \n",
        "    cond = tf.less_equal(x, x*0)\n",
        "    leaky = 0.01*x\n",
        "    return tf.where(cond, leaky, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnHdMT6R3u1l",
        "colab_type": "text"
      },
      "source": [
        "### Weight initialisation\n",
        "You can define and use your own weight and bias initialisation, in the same way you created your activation function, by using a simple python function. Here, let's initialise the weights in the dense layer using He's Normal Initialisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5zUxIejQ3oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_he_normal_init(shape):\n",
        "  stddev = tf.pow(2/shape[0], 0.5)\n",
        "  return tf.Variable(initial_value = tf.random.normal(shape=shape, mean=0,\n",
        "                                                      stddev=stddev,\n",
        "                                                      dtype=tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTTd-_d85BWo",
        "colab_type": "text"
      },
      "source": [
        "### Dense layer\n",
        "Let's now create the dense layer. This is a  [simple example](https://www.tensorflow.org/beta/tutorials/eager/custom_layers) of how to do this. The best way to implement your own layer is extending the tf.keras.Layer class. Let's go a step further and add some more flexibility to the layer being created.\n",
        "\n",
        "TODO :\n",
        "\n",
        "Parameterise the layer with the following:\n",
        "*   The number of output units.\n",
        "*   The kernal initialisation : Here, you can use the initialisation written above, or the default kernal initialisation in TensorFlow (Glorot).\n",
        "*   If a bias is needed.\n",
        "*   If an activation is required.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yYY278LdBcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_outputs, kernel_init = None, use_bias = False,\n",
        "               use_activation = False):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "    self.num_outputs = num_outputs\n",
        "    self.use_bias = use_bias\n",
        "    self.kernel_init = kernel_init\n",
        "    self.use_activation = use_activation\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    #Use the kernal initialisation written above\n",
        "    if self.kernel_init:\n",
        "      self.kernel = my_he_normal_init(shape= [int(input_shape[-1]), \n",
        "                                           self.num_outputs])\n",
        "    # Use the default initialisation\n",
        "    else:\n",
        "      self.kernel = self.add_variable(shape=[int(input_shape[-1]), \n",
        "                                           self.num_outputs])\n",
        "    if self.use_bias:\n",
        "      self.bias = my_he_normal_init(shape = [self.num_outputs])\n",
        "    \n",
        "  def call(self, input):\n",
        "    output = tf.matmul(input, self.kernel)\n",
        "    if self.use_bias:\n",
        "      output = output + self.bias\n",
        "    if self.use_activation:\n",
        "      output = my_leaky_relu(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hja2hJem9DjX",
        "colab_type": "text"
      },
      "source": [
        "### Define the model\n",
        "TensorFlow allows you to use the custom functions along with the inbuilt ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N6Qd-wrrgB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    # A 2D Convolution layer followed by a Pooling Layer\n",
        "    self.conv1 = tf.keras.layers.Conv2D(32, 3)\n",
        "    self.pool = tf.keras.layers.MaxPooling2D(pool_size = (2, 2))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # First use your dense layer with he normal initialisation, a bias and leaky relu activation.\n",
        "    self.dense1 = MyDenseLayer(128, kernel_init='he', use_bias=True,\n",
        "                               use_activation=True)\n",
        "    self.do = tf.keras.layers.Dropout(0.3)\n",
        "    # Again the dense layer, however, here using the default initialisation, no bias and no activation.\n",
        "    self.dense2 = MyDenseLayer(10)\n",
        "    #BatchNormalisation\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, x, training = True):\n",
        "    x = self.pool(self.conv1(x))\n",
        "    x = self.bn(x, training = training)\n",
        "    x = relu(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.do(x)\n",
        "    x = self.dense2(x)\n",
        "    x = softmax(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAF8yX4X25L5",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 : Define the loss function\n",
        "You can also write your own custom loss function, here are the mechanics of how to write your very own \"sparse_categorical_crossentropy\". \n",
        "\n",
        "Since the task here is performing multi-class classification, you will need to find the loss for each class label and sum the result. \n",
        "\n",
        "The expression of categorical_crossentropy for a single datapoint is : \n",
        "$$loss = -\\sum_{j=0}^{n}y_{true, j}*\\log(p_{pred, j})$$\n",
        "\n",
        "where\n",
        "\n",
        "1.   n : Number of classed\n",
        "2.   $y_{true}$ : True label of the data-point\n",
        "3.   $p_{pred}$ : Prediction probability vector for the data-point.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZjJECKpDTYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def myLoss(logits, labels, n_classes=10):  \n",
        "  labels = tf.squeeze(labels)\n",
        "  labels = tf.one_hot(labels, n_classes, dtype=tf.float32)\n",
        "  return tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(logits), axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5ITfV7gLPL6",
        "colab_type": "text"
      },
      "source": [
        "Define the metric to be used, which in the case of this tutorial is accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThzPytzt5Uc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "test_loss_function = tf.keras.metrics.SparseCategoricalCrossentropy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra83uALt5eZd",
        "colab_type": "text"
      },
      "source": [
        "Create the model and choose the optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs684KgT5iA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Model\n",
        "model =  MyModel()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV3Mpk7G51BX",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE-IEIkb7o6L",
        "colab_type": "text"
      },
      "source": [
        "You already know how to use tf.GradientTape to train the model. Check out this [tutorial](https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough) for a refresher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N0X-IEU7goo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = myLoss(logits, labels)\n",
        "  grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HgZGqNrLddw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_step(test_data):\n",
        "  test_loss_function.reset_states()\n",
        "  test_accuracy_metric.reset_states()\n",
        "  for (batch, (images, labels)) in enumerate(test_data):\n",
        "    test_predictions = model(images)\n",
        "    test_loss = test_loss_function(y_true=labels, y_pred=test_predictions)\n",
        "    test_acc = test_accuracy_metric(labels, test_predictions)\n",
        "  return test_loss, test_acc*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3GnlAL35xFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(5):\n",
        "  print('Epoch', epoch + 1)\n",
        "  for (batch, (images, labels)) in enumerate(cifar_train):\n",
        "    loss_value = train_on_batch(model, images, labels)\n",
        "    step = optimizer.iterations.numpy()\n",
        "    if step % 100 == 0:\n",
        "      print('Step %d\\tLoss: %.4f' % (step, loss_value))\n",
        "\n",
        "  test_loss, test_acc = test_step(cifar_test)\n",
        "  print(f'Test accuracy : {test_acc:.2f}\\nTest loss : {test_loss:.2f}\\n\\n')\n",
        "  \n",
        "print (\"Finished Training\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478QJIO7yME1",
        "colab_type": "text"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy8RDvGCC2sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = test_step(cifar_test)\n",
        "print(f'Test accuracy : {test_acc:.2f}\\nTest loss : {test_loss:.2f}\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHAocsxG8Hlq",
        "colab_type": "text"
      },
      "source": [
        "## Next steps\n",
        "Now that you know how to create such customizable neural networks, go ahead and implement complex architectures. You may try out different layers, activations or even loss functions."
      ]
    }
  ]
}